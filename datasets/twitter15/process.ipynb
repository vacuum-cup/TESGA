{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter2015/train.txt\",\"r\") as f:\n",
    "    num = 0\n",
    "    data = []\n",
    "    sample = {}\n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    for line in f:\n",
    "        num +=1\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # print(len(line),line)\n",
    "            if line.startswith(\"IMGID:\"):\n",
    "                img_id = line[len(\"IMGID:\"):]+\".jpg\"\n",
    "                sample = {\"img_id\":img_id}\n",
    "                words = []\n",
    "                tags = []\n",
    "            elif line.startswith(\"http:\") or line.startswith(\"https:\"):\n",
    "                continue\n",
    "            else:    \n",
    "                parts = line.split(\"\\t\")\n",
    "                words.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "        else:\n",
    "            if sample:\n",
    "                text = \" \".join(words)\n",
    "                ents = []\n",
    "                name = ''\n",
    "                tag = ''\n",
    "                pos = [0,0]\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    if tags[i].startswith(\"B-\"):\n",
    "                        if name:\n",
    "                            ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                            name = ''\n",
    "                            pos = [0,0]\n",
    "                            \n",
    "                        tag = tags[i][len(\"B-\"):]\n",
    "                        name = words[i]\n",
    "                        pos = [i,i+1]\n",
    "                    elif tags[i].startswith(\"I-\"):\n",
    "                        if name == '':\n",
    "                            continue\n",
    "                        name += \" \" +words[i]\n",
    "                        pos[1] = i+1\n",
    "                \n",
    "                if name:\n",
    "                    ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                \n",
    "                sample[\"text\"] = text \n",
    "                sample[\"ents\"] = ents\n",
    "                data.append(sample)\n",
    "                sample = {}\n",
    "            else:\n",
    "                print(\"error\")     \n",
    "        # if num >30:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 将列表写入JSON文件\n",
    "with open(\"twitter15_new/train.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file,indent=4)\n",
    "\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter2015/test.txt\",\"r\") as f:\n",
    "    data = []\n",
    "    sample = {}\n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # print(len(line),line)\n",
    "            if line.startswith(\"IMGID:\"):\n",
    "                img_id = line[len(\"IMGID:\"):]+\".jpg\"\n",
    "                sample = {\"img_id\":img_id}\n",
    "                words = []\n",
    "                tags = []\n",
    "            elif line.startswith(\"http:\") or line.startswith(\"https:\"):\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(\"\\t\")\n",
    "                words.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "        else:\n",
    "            if sample:\n",
    "                text = \" \".join(words)\n",
    "                ents = []\n",
    "                name = ''\n",
    "                tag = ''\n",
    "                pos = [0,0]\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    if tags[i].startswith(\"B-\"):\n",
    "                        if name:\n",
    "                            ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                            name = ''\n",
    "                            pos = [0,0]\n",
    "                            \n",
    "                        tag = tags[i][len(\"B-\"):]\n",
    "                        name = words[i]\n",
    "                        pos = [i,i+1]\n",
    "                    elif tags[i].startswith(\"I-\"):\n",
    "                        if name == '':\n",
    "                            continue\n",
    "                        name += \" \" +words[i]\n",
    "                        pos[1] = i+1\n",
    "                \n",
    "                if name:\n",
    "                    ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                \n",
    "                sample[\"text\"] = text \n",
    "                sample[\"ents\"] = ents\n",
    "                data.append(sample)\n",
    "                sample = {}\n",
    "            else:\n",
    "                print(\"error\")   \n",
    "                \n",
    "                \n",
    "with open(\"twitter15_new/test.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file,indent=4)\n",
    "\n",
    "json_file.close()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3257"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter2015/valid.txt\",\"r\") as f:\n",
    "    data = []\n",
    "    sample = {}\n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # print(len(line),line)\n",
    "            if line.startswith(\"IMGID:\"):\n",
    "                img_id = line[len(\"IMGID:\"):]+\".jpg\"\n",
    "                sample = {\"img_id\":img_id}\n",
    "                words = []\n",
    "                tags = []\n",
    "            elif line.startswith(\"http:\") or line.startswith(\"https:\"):\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(\"\\t\")\n",
    "                words.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "        else:\n",
    "            if sample:\n",
    "                text = \" \".join(words)\n",
    "                ents = []\n",
    "                name = ''\n",
    "                tag = ''\n",
    "                pos = [0,0]\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    if tags[i].startswith(\"B-\"):\n",
    "                        if name:\n",
    "                            ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                            name = ''\n",
    "                            pos = [0,0]\n",
    "                            \n",
    "                        tag = tags[i][len(\"B-\"):]\n",
    "                        name = words[i]\n",
    "                        pos = [i,i+1]\n",
    "                    elif tags[i].startswith(\"I-\"):\n",
    "                        if name == '':\n",
    "                            continue\n",
    "                        name += \" \" +words[i]\n",
    "                        pos[1] = i+1\n",
    "                \n",
    "                if name:\n",
    "                    ents.append({\"name\":name,\"pos\":pos,\"tag\":tag})\n",
    "                \n",
    "                sample[\"text\"] = text \n",
    "                sample[\"ents\"] = ents\n",
    "                data.append(sample)\n",
    "                sample = {}\n",
    "            else:\n",
    "                print(\"error\")   \n",
    "                \n",
    "                \n",
    "with open(\"twitter15_new/valid.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file,indent=4)\n",
    "\n",
    "json_file.close()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在文件中加上  BLIP 评估的img与text相似度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import sys\n",
    "sys.path.append('../../Blip/')  \n",
    "from models.blip_itm import blip_itm\n",
    "\n",
    "\n",
    "def load_demo_image(image_size,img_path,device):\n",
    "    \n",
    "    raw_image = Image.open(img_path).convert(\"RGB\") \n",
    "\n",
    "    w,h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/users/wgx/Mult_m_NRE/multi-joint-ner-re/Blip/MODELS/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "image_size = 384\n",
    "\n",
    "model_url = '/home/users/wgx/Mult_m_NRE/multi-joint-ner-re/Blip/MODELS/model_base_retrieval_coco.pth'\n",
    "    \n",
    "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据条数： 4000\n",
      "每条数据的样式： dict_keys(['img_id', 'text', 'ents']) \n",
      " {'img_id': '1015799.jpg', 'text': 'RT @JayKenMinaj _ : Me outside of where George Zimmerman got shot at . You know God is so good .', 'ents': [{'name': 'George Zimmerman', 'pos': [8, 10], 'tag': 'PER'}]}\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "train_data = json.load(open(\"twitter15_new/train.json\",'r'))\n",
    "\n",
    "print(\"训练集数据条数：\",len(train_data))\n",
    "print(\"每条数据的样式：\",train_data[0].keys(),\"\\n\",train_data[0])\n",
    "\n",
    "new_train_data = []\n",
    "file_path = \"twitter2015_images/\"\n",
    "image_size = 384\n",
    "\n",
    "num = 0\n",
    "\n",
    "for td in train_data:\n",
    "    num+=1\n",
    "    sample = td\n",
    "    \n",
    "    caption = td[\"text\"]\n",
    "    img_path = file_path + td[\"img_id\"]\n",
    "    \n",
    "    try:\n",
    "        image = load_demo_image(image_size=image_size,img_path=img_path, device=device)\n",
    "        \n",
    "        \n",
    "        itm_output = model(image,caption,match_head='itm')\n",
    "        itm_score = torch.nn.functional.softmax(itm_output,dim=1)[:,1]\n",
    "        # print('The image and text is matched with a probability of %.4f'%itm_score)\n",
    "\n",
    "        itc_score = model(image,caption,match_head='itc')\n",
    "        # print('The image feature and text feature has a cosine similarity of %.4f'%itc_score)\n",
    "        \n",
    "        sample[\"itm_score\"] = itm_score.cpu().detach().item()\n",
    "        sample[\"itc_score\"] = itc_score.cpu().detach().item()\n",
    "        \n",
    "        # print(sample)\n",
    "        new_train_data.append(sample)\n",
    "        # break\n",
    "        if num %100 ==0:\n",
    "            print(num)\n",
    "    except:\n",
    "        print(img_path)\n",
    "    \n",
    "with open(\"twitter15_new2/new_train.json\", 'w') as json_file:\n",
    "    json.dump(new_train_data, json_file,indent=4)\n",
    "    \n",
    "json_file.close()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据条数： 3257\n",
      "每条数据的样式： dict_keys(['img_id', 'text', 'ents']) \n",
      " {'img_id': '62654.jpg', 'text': 'RT @ThePatriot143 : TIMELINE SHOWS HOW CLINTONS TOOK $ 1 . 8 MILLION FROM KEYSTONE PIPELINE INVESTORS #ClintonCash', 'ents': [{'name': 'CLINTONS', 'pos': [6, 7], 'tag': 'ORG'}, {'name': 'KEYSTONE', 'pos': [14, 15], 'tag': 'ORG'}]}\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "test_data = json.load(open(\"twitter15_new/test.json\",'r'))\n",
    "\n",
    "print(\"训练集数据条数：\",len(test_data))\n",
    "print(\"每条数据的样式：\",test_data[0].keys(),\"\\n\",test_data[0])\n",
    "\n",
    "new_test_data = []\n",
    "file_path = \"twitter2015_images/\"\n",
    "image_size = 384\n",
    "\n",
    "num = 0\n",
    "\n",
    "for td in test_data:\n",
    "    num+=1\n",
    "    sample = td\n",
    "    \n",
    "    caption = td[\"text\"]\n",
    "    img_path = file_path + td[\"img_id\"]\n",
    "    try:\n",
    "        image = load_demo_image(image_size=image_size,img_path=img_path, device=device)\n",
    "        \n",
    "        \n",
    "        itm_output = model(image,caption,match_head='itm')\n",
    "        itm_score = torch.nn.functional.softmax(itm_output,dim=1)[:,1]\n",
    "        # print('The image and text is matched with a probability of %.4f'%itm_score)\n",
    "\n",
    "        itc_score = model(image,caption,match_head='itc')\n",
    "        # print('The image feature and text feature has a cosine similarity of %.4f'%itc_score)\n",
    "        \n",
    "        sample[\"itm_score\"] = itm_score.cpu().detach().item()\n",
    "        sample[\"itc_score\"] = itc_score.cpu().detach().item()\n",
    "        \n",
    "        # print(sample)\n",
    "        new_test_data.append(sample)\n",
    "        # break\n",
    "        if num %100 ==0:\n",
    "            print(num)\n",
    "    except:\n",
    "        print(img_path)\n",
    "    \n",
    "with open(\"twitter15_new2/new_test.json\", 'w') as json_file:\n",
    "    json.dump(new_test_data, json_file,indent=4)\n",
    "    \n",
    "json_file.close()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3257"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据条数： 1000\n",
      "每条数据的样式： dict_keys(['img_id', 'text', 'ents']) \n",
      " {'img_id': '32977.jpg', 'text': 'RT @jonathanchait : How post-Katrina New Orleans proved urban education reform can work', 'ents': [{'name': 'New Orleans', 'pos': [5, 7], 'tag': 'LOC'}]}\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "valid_data = json.load(open(\"twitter15_new/valid.json\",'r'))\n",
    "\n",
    "print(\"训练集数据条数：\",len(valid_data))\n",
    "print(\"每条数据的样式：\",valid_data[0].keys(),\"\\n\",valid_data[0])\n",
    "\n",
    "new_valid_data = []\n",
    "file_path = \"twitter2015_images/\"\n",
    "image_size = 384\n",
    "\n",
    "num = 0\n",
    "\n",
    "for td in valid_data:\n",
    "    num+=1\n",
    "    sample = td\n",
    "    \n",
    "    caption = td[\"text\"]\n",
    "    img_path = file_path + td[\"img_id\"]\n",
    "    try:\n",
    "        image = load_demo_image(image_size=image_size,img_path=img_path, device=device)\n",
    "        \n",
    "        \n",
    "        itm_output = model(image,caption,match_head='itm')\n",
    "        itm_score = torch.nn.functional.softmax(itm_output,dim=1)[:,1]\n",
    "        # print('The image and text is matched with a probability of %.4f'%itm_score)\n",
    "\n",
    "        itc_score = model(image,caption,match_head='itc')\n",
    "        # print('The image feature and text feature has a cosine similarity of %.4f'%itc_score)\n",
    "        \n",
    "        sample[\"itm_score\"] = itm_score.cpu().detach().item()\n",
    "        sample[\"itc_score\"] = itc_score.cpu().detach().item()\n",
    "        \n",
    "        # print(sample)\n",
    "        new_valid_data.append(sample)\n",
    "        # break\n",
    "        if num %100 ==0:\n",
    "            print(num)\n",
    "    except:\n",
    "        print(img_path)\n",
    "    \n",
    "with open(\"twitter15_new2/new_valid.json\", 'w') as json_file:\n",
    "    json.dump(new_valid_data, json_file,indent=4)\n",
    "    \n",
    "json_file.close()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_valid_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mult",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
